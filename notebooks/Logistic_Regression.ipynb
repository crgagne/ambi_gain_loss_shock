{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../functions/\")\n",
    "#for data preprocessing\n",
    "import Scripts_Data_Processing\n",
    "import imp\n",
    "\n",
    "imp.reload(Scripts_Data_Processing)\n",
    "from Scripts_Data_Processing import *\n",
    "#for model fit\n",
    "import Scripts_LogRegModels_v2\n",
    "imp.reload(Scripts_LogRegModels_v2)\n",
    "from Scripts_LogRegModels_v2 import *\n",
    "\n",
    "import NoBrainer_Analysis_AllinOne\n",
    "imp.reload( NoBrainer_Analysis_AllinOne)\n",
    "from  NoBrainer_Analysis_AllinOne import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first version of the function without bars being split\n",
    "def plot_params_first_version(df,stripplot=False,outlier_cutoff=None,task='shock'):\n",
    "    plt.style.use(['seaborn-white', 'seaborn-paper'])\n",
    "    matplotlib.rc(\"font\", family=\"Times New Roman\")\n",
    "    sns.set_context('talk')\n",
    "    sns.set_style('white',{'figure.facecolor':'white'})\n",
    "    \n",
    "    \n",
    "    if outlier_cutoff is not None:\n",
    "        df = df[(df.beta>-1.0*outlier_cutoff)&(df.beta<outlier_cutoff)]\n",
    "    \n",
    "    axis = sns.barplot(x='parameter',y='beta',data=df,color=[.4,.4,.4],ci=95,alpha=0.4)\n",
    "    \n",
    "    if stripplot:\n",
    "        sns.stripplot(x=\"parameter\", y=\"beta\", data=df,color=[.4,.4,.4],alpha=0.2,jitter=True);\n",
    "\n",
    "        \n",
    "    current_palette=sns.color_palette()\n",
    "    fig = plt.gcf()\n",
    "    fig.suptitle('Model Parameters: '+modelname+' '+task,fontsize=12,x=0.55)\n",
    "    sns.despine(ax=axis)\n",
    "    axis.set_ylabel('beta (Prob Choose Right (except on Ambig))',fontsize=12)\n",
    "    axis.set_xlabel('parameter',fontsize=12)\n",
    "    axis.set_xticklabels(df.parameter.unique(),rotation=45,fontsize=12,ha='right')\n",
    "    axis = plt.gca()\n",
    "    \n",
    "    # change name if needed\n",
    "    xlabels = axis.get_xticklabels()\n",
    "\n",
    "    fig.suptitle('')\n",
    "    axis.set_title('Model Parameters (Across all Subjects)')\n",
    "    axis.set_xlabel('Parameter')\n",
    "    axis.set_ylabel('Group Regression Coefficients \\n (Probability Choosing Ambig)')\n",
    "    plt.tight_layout()\n",
    "    return(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# second version of the function with bar graphs being split\n",
    "def plot_params(df,stripplot=False,outlier_cutoff=None):\n",
    "    plt.style.use(['seaborn-white', 'seaborn-paper'])\n",
    "    matplotlib.rc(\"font\", family=\"Times New Roman\")\n",
    "    sns.set_context('talk')\n",
    "    sns.set_style('white',{'figure.facecolor':'white'})\n",
    "    \n",
    "    \n",
    "    if outlier_cutoff is not None:\n",
    "        df = df[(df.beta>-1.0*outlier_cutoff)&(df.beta<outlier_cutoff)]\n",
    "    \n",
    "    axis = sns.barplot(x='parameter',y='beta',hue='split',data=df,ci=95,alpha=0.4)\n",
    "    \n",
    "    if stripplot:\n",
    "        sns.stripplot(x=\"parameter\", y=\"beta\",hue='split', data=df,alpha=0.2,jitter=True);\n",
    "\n",
    "    current_palette=sns.color_palette()\n",
    "    fig = plt.gcf()\n",
    "    fig.suptitle('Model Parameters:',fontsize=12,x=0.55)\n",
    "    sns.despine(ax=axis)\n",
    "    axis.set_ylabel('beta (Prob Choose Right (except on Ambig))',fontsize=12)\n",
    "    axis.set_xlabel('parameter',fontsize=12)\n",
    "    axis.set_xticklabels(df.parameter.unique(),rotation=45,fontsize=12,ha='right')\n",
    "    axis = plt.gca()\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "    \n",
    "    # change name if needed\n",
    "    #xlabels = axis.get_xticklabels()\n",
    "\n",
    "    #fig.suptitle('')\n",
    "    #axis.set_title('Model Parameters (Across all Subjects)')\n",
    "    #axis.set_xlabel('Parameter')\n",
    "    #axis.set_ylabel('Group Regression Coefficients \\n (Probability Choosing Ambig)')\n",
    "    #plt.tight_layout()\n",
    "    return(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gainloss load in single subjectdata + nobrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../functions/NoBrainer_Analysis_AllinOne.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df['left_better']=left_better\n",
      "../functions/NoBrainer_Analysis_AllinOne.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df['right_better']=right_better\n",
      "../functions/NoBrainer_Analysis_AllinOne.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df['choseBetter'] = (df['resp'] == 'left') & (df['left_better']== True) | (df['resp'] == 'right') & (df['right_better']==True)\n",
      "../functions/NoBrainer_Analysis_AllinOne.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df['noBrainer'] = (df['right_better'] != df['left_better'])\n"
     ]
    }
   ],
   "source": [
    "df_list = []\n",
    "vp_perform_gainloss_list = []\n",
    "vp_nb_gainloss_list = []\n",
    "vp_list = ['06', '07', '10', '11', '12', '13', '15', '16', '17', '18', '19', '20', '22', '23_2', '25_2', '26_2', '27_2', '28_2', '29', '30']\n",
    "for vp in vp_list:\n",
    "    path = os.path.join(os.getcwd(),'..','data','data_gainloss_logfiles','vp' + vp + '_gainloss_processed.csv')\n",
    "    df = pd.read_csv(path, sep=\",\")\n",
    "    #preprocess gainloss data\n",
    "    df=preprocess_gainloss(df)\n",
    "    #preprocess data\n",
    "    df = preprocess(df)\n",
    "    #store prepocessed data in list that contains data for all subjects (for later analysis)\n",
    "    df_list.append(df)\n",
    "    #create subset with unambiguous trials for no brainer analysis\n",
    "    nb_df = drop_ambi_trials(df)\n",
    "    #create variables indicating whether left or right was the better option\n",
    "    better_choice_gainloss(nb_df)\n",
    "    #indicate whether the better box was chosen\n",
    "    nb_df = right_choice(nb_df)\n",
    "    #only keep trials that are 'no brainers'\n",
    "    nb_df = keep_nobrainers(nb_df)\n",
    "    #calculate performance\n",
    "    vp_perform_gainloss = ['vp' + vp, vp_perf(nb_df)]\n",
    "    #store each vp performance in list\n",
    "    vp_perform_gainloss_list.append(vp_perform_gainloss)\n",
    "    #vp performance sectionwise\n",
    "    vp_nb_gainloss = nb_df.groupby('section').mean().add_prefix('gainloss_')[['gainloss_choseBetter']]\n",
    "    vp_nb_gainloss['MID'] = 'vp'+ vp\n",
    "    vp_nb_gainloss_list.append(vp_nb_gainloss)\n",
    "    \n",
    "#make dataframe for nb performance\n",
    "nobrainer_gainloss = pd.DataFrame(vp_perform_gainloss_list,columns=['MID','nbperf'])\n",
    "\n",
    "#dataframe for single subject (last one) to try out code\n",
    "gainloss_ls_df = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[         gainloss_choseBetter   MID\n",
       " section                            \n",
       " 1                    0.888889  vp06\n",
       " 2                    1.000000  vp06\n",
       " 3                    0.928571  vp06,          gainloss_choseBetter   MID\n",
       " section                            \n",
       " 1                    0.888889  vp07\n",
       " 2                    1.000000  vp07\n",
       " 3                    1.000000  vp07,          gainloss_choseBetter   MID\n",
       " section                            \n",
       " 1                         1.0  vp10\n",
       " 2                         1.0  vp10\n",
       " 3                         1.0  vp10,          gainloss_choseBetter   MID\n",
       " section                            \n",
       " 1                    0.888889  vp11\n",
       " 2                    1.000000  vp11\n",
       " 3                    1.000000  vp11,          gainloss_choseBetter   MID\n",
       " section                            \n",
       " 1                    1.000000  vp12\n",
       " 2                    0.909091  vp12\n",
       " 3                    0.928571  vp12,          gainloss_choseBetter   MID\n",
       " section                            \n",
       " 1                    1.000000  vp13\n",
       " 2                    1.000000  vp13\n",
       " 3                    0.785714  vp13,          gainloss_choseBetter   MID\n",
       " section                            \n",
       " 1                    1.000000  vp15\n",
       " 2                    0.909091  vp15\n",
       " 3                    0.928571  vp15,          gainloss_choseBetter   MID\n",
       " section                            \n",
       " 1                    1.000000  vp16\n",
       " 2                    1.000000  vp16\n",
       " 3                    0.928571  vp16,          gainloss_choseBetter   MID\n",
       " section                            \n",
       " 1                    0.888889  vp17\n",
       " 2                    1.000000  vp17\n",
       " 3                    1.000000  vp17,          gainloss_choseBetter   MID\n",
       " section                            \n",
       " 1                    0.888889  vp18\n",
       " 2                    1.000000  vp18\n",
       " 3                    0.928571  vp18,          gainloss_choseBetter   MID\n",
       " section                            \n",
       " 1                    1.000000  vp19\n",
       " 2                    0.818182  vp19\n",
       " 3                    0.285714  vp19,          gainloss_choseBetter   MID\n",
       " section                            \n",
       " 1                    0.888889  vp20\n",
       " 2                    1.000000  vp20\n",
       " 3                    1.000000  vp20,          gainloss_choseBetter   MID\n",
       " section                            \n",
       " 1                    0.888889  vp22\n",
       " 2                    0.909091  vp22\n",
       " 3                    0.857143  vp22,          gainloss_choseBetter     MID\n",
       " section                              \n",
       " 1                    0.888889  vp23_2\n",
       " 2                    1.000000  vp23_2\n",
       " 3                    1.000000  vp23_2,          gainloss_choseBetter     MID\n",
       " section                              \n",
       " 1                         1.0  vp25_2\n",
       " 2                         1.0  vp25_2\n",
       " 3                         1.0  vp25_2,          gainloss_choseBetter     MID\n",
       " section                              \n",
       " 1                    1.000000  vp26_2\n",
       " 2                    1.000000  vp26_2\n",
       " 3                    0.928571  vp26_2,          gainloss_choseBetter     MID\n",
       " section                              \n",
       " 1                         1.0  vp27_2\n",
       " 2                         1.0  vp27_2\n",
       " 3                         1.0  vp27_2,          gainloss_choseBetter     MID\n",
       " section                              \n",
       " 1                    0.777778  vp28_2\n",
       " 2                    0.909091  vp28_2\n",
       " 3                    0.857143  vp28_2,          gainloss_choseBetter   MID\n",
       " section                            \n",
       " 1                    0.666667  vp29\n",
       " 2                    0.545455  vp29\n",
       " 3                    0.500000  vp29,          gainloss_choseBetter   MID\n",
       " section                            \n",
       " 1                    0.888889  vp30\n",
       " 2                    0.909091  vp30\n",
       " 3                    0.642857  vp30]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vp_nb_gainloss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create preprocessed dataframe with all vp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Merge dataframe list to single dataframe. \"inner\": Just take columns which exist in all dataframes    \n",
    "gainloss_df = pd.concat(df_list, ignore_index = True, join = 'inner')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gainloss_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gain/Loss Model fit - individual subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_sets_split = {}\n",
    "param_sets_split['0'] = ['mag_diff','prob_diff']\n",
    "param_sets_split['1'] = ['mag_diff','prob_diff','sqrt_prop_revealed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vp_list = ['06','07','10', '12', '13', '15', '16', '17', '18', '19', '20', '22', '23_2', '25_2', '26_2', '27_2', '28_2', '29', '30']\n",
    "resultnames = ['bic','aic','pseudoR2','pred_acc']\n",
    "model_param_df = np.array(['','','',3.0])\n",
    "model_summary_df = nobrainer_gainloss\n",
    "\n",
    "for vp in vp_list:\n",
    "    \n",
    "    df = gainloss_df[gainloss_df.MID == 'vp' + vp]\n",
    "    MID = 'vp' + vp\n",
    "        \n",
    "    for split in ['ambig_gain','ambig_loss','unambig_gain','unambig_loss']:\n",
    "        #print(split)\n",
    "        if 'unambig' in split:\n",
    "            paramnum='0'\n",
    "        else:\n",
    "            paramnum='1'\n",
    "        # Fit a model to the ambiguous gain trials \n",
    "        out = fit_model_split_amb_unamb_gain_loss(df, whichreturn= split, params=param_sets_split[paramnum],zscore=False)\n",
    "        \n",
    "    \n",
    "        modelname = out['modelname']\n",
    "\n",
    "        for result in resultnames:\n",
    "            model_summary_df.loc[(model_summary_df.MID== 'vp' + vp),result+'_'+split]=out[result]\n",
    "\n",
    "        params = out['params']\n",
    "        for param in params.index:\n",
    "            paramn = param.replace('_loss','')\n",
    "            paramn = paramn.replace('_gain','')\n",
    "            paramn = paramn.replace('_amb','')\n",
    "            paramn = paramn.replace('_rl','')\n",
    "            row = np.array([MID,paramn,split,params[param]])\n",
    "            model_param_df=np.vstack((model_param_df,row))\n",
    "\n",
    "model_param_df = pd.DataFrame(model_param_df,columns=['MID','parameter','split','beta'])\n",
    "model_param_df.drop(0,inplace=True) #df.index[0]\n",
    "model_param_df['beta']=model_param_df['beta'].astype('float')\n",
    "model_param_df_gainloss = model_param_df\n",
    "\n",
    "print(out['aic'])\n",
    "print(out['pred_acc'])\n",
    "print(out['modelname'])\n",
    "out['results'].summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Plot gain loss data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.style.use(['seaborn-white', 'seaborn-paper'])\n",
    "matplotlib.rc(\"font\", family=\"Times New Roman\")\n",
    "sns.set_context('talk')\n",
    "fig,axes =plt.subplots(1,2, figsize=(12,4))\n",
    "axes[0].scatter(model_summary_df.nbperf,model_summary_df.pseudoR2_ambig_gain,label='gain trials', c='b')\n",
    "axes[0].scatter(model_summary_df.nbperf,model_summary_df.pseudoR2_ambig_loss,label='loss trials', c='r')\n",
    "sns.despine()\n",
    "axes[0].set_xlabel('no brainer perf')\n",
    "axes[0].set_ylabel('PseudoR2 of model fit')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_params(model_param_df_gainloss, stripplot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## ask Chris about llr_pvalue\n",
    "#axes[1].scatter(model_summary_df.nbperf,model_summary_df.llr_pvalue_ambig_gain,label='gain trials')\n",
    "#axes[1].scatter(model_summary_df.nbperf,model_summary_df.llr_pvalue_ambig_loss,label='loss trials')\n",
    "#sns.despine()\n",
    "#axes[1].set_xlabel('no brainer perf')\n",
    "#axes[1].set_ylabel('p-value of model fit')\n",
    "#plt.legend()\n",
    "#axes[1].axhline(y=0.05,linestyle='--',color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# shock load in single subjectdata + nobrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_list = []\n",
    "vp_list = ['06', '07', '10', '11', '12', '13', '15', '16', '17', '18', '19', '20', '22', '23', '25', '26', '27', '28', '29', '30']\n",
    "vp_perform_shock_list = []\n",
    "vp_nb_shock_list = []\n",
    "section_list = ['1', '2', '3']\n",
    "for vp in vp_list:\n",
    "    df = []\n",
    "    for sec in section_list:\n",
    "        path = os.path.join(os.getcwd(),'..','data','data_shock_logfiles','Expt1Pain_Behaviour_vp' + vp + '_' + sec + '.txt')\n",
    "        df_dummy = pd.read_csv(path, sep=\"\\t\", skiprows = [0])\n",
    "        df_dummy = df_dummy[:-1] #deletes last row of each section as it does not contain trial data\n",
    "        df_dummy['MID'] = 'vp'+ vp\n",
    "        df_dummy['section'] = sec\n",
    "        df_dummy.columns = df_dummy.columns.str.replace(' ','')\n",
    "        df.append(df_dummy)\n",
    "    \n",
    "    #create a df that contains data from all sections    \n",
    "    df = pd.concat(df, ignore_index = True, join = 'inner')\n",
    "    #preprocess shock data\n",
    "    df = preprocess_shock(df)\n",
    "    #preprocess data\n",
    "    df = preprocess(df)\n",
    "    #store prepocessed data in list that contains data for all subjects (for later analysis)\n",
    "    df_list.append(df)\n",
    "    #create subset with unambiguous trials for no brainer analysis\n",
    "    nb_df = drop_ambi_trials(df)\n",
    "    #create variables indicating whether left or right was the better option\n",
    "    better_choice_shock(nb_df)\n",
    "    #indicate whether the better box was chosen\n",
    "    nb_df = right_choice(nb_df)\n",
    "    #only keep trials that are 'no brainers'\n",
    "    nb_df = keep_nobrainers(nb_df)\n",
    "    #calculate performance\n",
    "    vp_perform_shock = ['vp' + vp, vp_perf(nb_df)]\n",
    "    #store each vp performance in list\n",
    "    vp_perform_shock_list.append(vp_perform_shock)\n",
    "    #vp performance sectionwise\n",
    "    vp_nb_shock = nb_df.groupby('section').mean().add_prefix('shock_')[['shock_choseBetter']]\n",
    "    vp_nb_shock['MID'] = 'vp'+ vp\n",
    "    vp_nb_shock_list.append(vp_nb_shock)\n",
    "    \n",
    "#make dataframe for nb performance\n",
    "nobrainer_shock = pd.DataFrame(vp_perform_shock_list,columns=['MID','nbperf'])\n",
    "\n",
    "#dataframe for single subject (last one) to try out code\n",
    "shock_ls_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vp_nb_shock_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create preprocessed dataframe with all vp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create complete df for shock condition with all vps        \n",
    "shock_df = pd.concat(df_list, ignore_index = True, join = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shock: Model fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vp_list = ['06', '07', '10', '11' ,'12', '13', '15', '16', '17', '18', '19', '20', '22', '23', '25', '26', '27', '28', '29', '30']\n",
    "resultnames = ['bic','aic','pseudoR2','pred_acc'] ##'llr_pvalue' is missing bc did not work\n",
    "model_param_df = np.array(['','','',3.0])\n",
    "model_summary_df = nobrainer_shock\n",
    "\n",
    "for vp in vp_list:\n",
    "    \n",
    "    df = shock_df[shock_df.MID == 'vp' + vp]\n",
    "    MID = 'vp' + vp\n",
    "        \n",
    "    for split in ['ambig_shock','unambig_shock']:\n",
    "        #print(split)\n",
    "        if 'unambig' in split:\n",
    "            paramnum='0'\n",
    "        else:\n",
    "            paramnum='1'\n",
    "        # Fit a model to the ambiguous gain trials \n",
    "        out = fit_model_split_amb_unamb_gain_loss(df, whichreturn= split, params=param_sets_split[paramnum],zscore=True)\n",
    "        \n",
    "    \n",
    "        modelname = out['modelname']\n",
    "\n",
    "        for result in resultnames:\n",
    "            model_summary_df.loc[(model_summary_df.MID== 'vp' + vp),result+'_'+split]=out[result]\n",
    "\n",
    "        params = out['params']\n",
    "        for param in params.index:\n",
    "            paramn = param.replace('_shock','')\n",
    "            paramn = paramn.replace('_amb','')\n",
    "            paramn = paramn.replace('_rl','')\n",
    "            row = np.array([MID,paramn,split,params[param]])\n",
    "            model_param_df=np.vstack((model_param_df,row))\n",
    "\n",
    "model_param_df = pd.DataFrame(model_param_df,columns=['MID','parameter','split','beta'])\n",
    "model_param_df.drop(0,inplace=True) #df.index[0]\n",
    "model_param_df['beta']=model_param_df['beta'].astype('float')\n",
    "model_param_df_shock = model_param_df        \n",
    "\n",
    "\n",
    "print(out['aic'])\n",
    "print(out['pred_acc'])\n",
    "print(out['modelname'])\n",
    "out['results'].summary()\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot shock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.style.use(['seaborn-white', 'seaborn-paper'])\n",
    "matplotlib.rc(\"font\", family=\"Times New Roman\")\n",
    "sns.set_context('talk')\n",
    "fig,axes =plt.subplots(1,2, figsize=(12,4))\n",
    "axes[0].scatter(model_summary_df.nbperf,model_summary_df.pseudoR2_ambig_shock,label='shock trials', c='b')\n",
    "sns.despine()\n",
    "axes[0].set_xlabel('no brainer perf')\n",
    "axes[0].set_ylabel('PseudoR2 of model fit')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_params(model_param_df_shock, stripplot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "523px",
    "left": "0px",
    "right": "1192.45px",
    "top": "107px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
