{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../functions/\")\n",
    "#for data preprocessing\n",
    "import Scripts_Data_Processing\n",
    "import imp\n",
    "\n",
    "imp.reload(Scripts_Data_Processing)\n",
    "from Scripts_Data_Processing import *\n",
    "#for model fit\n",
    "import Scripts_LogRegModels_v2\n",
    "imp.reload(Scripts_LogRegModels_v2)\n",
    "from Scripts_LogRegModels_v2 import *\n",
    "\n",
    "import NoBrainer_Analysis_AllinOne\n",
    "imp.reload( NoBrainer_Analysis_AllinOne)\n",
    "from  NoBrainer_Analysis_AllinOne import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gainloss load in single subjectdata + nobrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../functions/NoBrainer_Analysis_AllinOne.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df['left_better']=left_better\n",
      "../functions/NoBrainer_Analysis_AllinOne.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df['right_better']=right_better\n",
      "../functions/NoBrainer_Analysis_AllinOne.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df['choseBetter'] = (df['resp'] == 'left') & (df['left_better']== True) | (df['resp'] == 'right') & (df['right_better']==True)\n",
      "../functions/NoBrainer_Analysis_AllinOne.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df['noBrainer'] = (df['right_better'] != df['left_better'])\n"
     ]
    }
   ],
   "source": [
    "df_list = []\n",
    "vp_perform_gainloss_list = []\n",
    "vp_nb_gainloss_list = []\n",
    "vp_list = ['06', '07', '10', '11', '12', '13', '15', '16', '17', '18', '19', '20', '22'] #'23_2', '25_2', '26_2', '27_2', '28_2', '29', '30']\n",
    "for vp in vp_list:\n",
    "    path = os.path.join(os.getcwd(),'..','data','data_gainloss_logfiles','vp' + vp + '_gainloss_processed.csv')\n",
    "    df = pd.read_csv(path, sep=\",\")\n",
    "    #preprocess gainloss data\n",
    "    df=preprocess_gainloss(df)\n",
    "    #preprocess data\n",
    "    df = preprocess(df)\n",
    "    #store prepocessed data in list that contains data for all subjects (for later analysis)\n",
    "    df_list.append(df)\n",
    "    #create subset with unambiguous trials for no brainer analysis\n",
    "    nb_df = drop_ambi_trials(df)\n",
    "    #create variables indicating whether left or right was the better option\n",
    "    better_choice_gainloss(nb_df)\n",
    "    #indicate whether the better box was chosen\n",
    "    nb_df = right_choice(nb_df)\n",
    "    #only keep trials that are 'no brainers'\n",
    "    nb_df = keep_nobrainers(nb_df)\n",
    "    #calculate performance\n",
    "    vp_perform_gainloss = ['vp' + vp, vp_perf(nb_df)]\n",
    "    #store each vp performance in list\n",
    "    vp_perform_gainloss_list.append(vp_perform_gainloss)\n",
    "    #vp performance sectionwise\n",
    "    vp_nb_gainloss = nb_df.groupby('section').mean().add_prefix('gainloss_')[['gainloss_choseBetter']]\n",
    "    vp_nb_gainloss['MID'] = 'vp'+ vp\n",
    "    vp_nb_gainloss_list.append(vp_nb_gainloss)\n",
    "    \n",
    "#make dataframe for nb performance\n",
    "nobrainer_gainloss = pd.DataFrame(vp_perform_gainloss_list,columns=['MID','nbperf'])\n",
    "\n",
    "#dataframe for single subject (last one) to try out code\n",
    "gainloss_ls_df = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[         gainloss_choseBetter   MID\n",
       " section                            \n",
       " 1                    0.888889  vp06\n",
       " 2                    1.000000  vp06\n",
       " 3                    0.928571  vp06,          gainloss_choseBetter   MID\n",
       " section                            \n",
       " 1                    0.888889  vp07\n",
       " 2                    1.000000  vp07\n",
       " 3                    1.000000  vp07,          gainloss_choseBetter   MID\n",
       " section                            \n",
       " 1                         1.0  vp10\n",
       " 2                         1.0  vp10\n",
       " 3                         1.0  vp10,          gainloss_choseBetter   MID\n",
       " section                            \n",
       " 1                    0.888889  vp11\n",
       " 2                    1.000000  vp11\n",
       " 3                    1.000000  vp11,          gainloss_choseBetter   MID\n",
       " section                            \n",
       " 1                    1.000000  vp12\n",
       " 2                    0.909091  vp12\n",
       " 3                    0.928571  vp12,          gainloss_choseBetter   MID\n",
       " section                            \n",
       " 1                    1.000000  vp13\n",
       " 2                    1.000000  vp13\n",
       " 3                    0.785714  vp13,          gainloss_choseBetter   MID\n",
       " section                            \n",
       " 1                    1.000000  vp15\n",
       " 2                    0.909091  vp15\n",
       " 3                    0.928571  vp15,          gainloss_choseBetter   MID\n",
       " section                            \n",
       " 1                    1.000000  vp16\n",
       " 2                    1.000000  vp16\n",
       " 3                    0.928571  vp16,          gainloss_choseBetter   MID\n",
       " section                            \n",
       " 1                    0.888889  vp17\n",
       " 2                    1.000000  vp17\n",
       " 3                    1.000000  vp17,          gainloss_choseBetter   MID\n",
       " section                            \n",
       " 1                    0.888889  vp18\n",
       " 2                    1.000000  vp18\n",
       " 3                    0.928571  vp18,          gainloss_choseBetter   MID\n",
       " section                            \n",
       " 1                    1.000000  vp19\n",
       " 2                    0.818182  vp19\n",
       " 3                    0.285714  vp19,          gainloss_choseBetter   MID\n",
       " section                            \n",
       " 1                    0.888889  vp20\n",
       " 2                    1.000000  vp20\n",
       " 3                    1.000000  vp20,          gainloss_choseBetter   MID\n",
       " section                            \n",
       " 1                    0.888889  vp22\n",
       " 2                    0.909091  vp22\n",
       " 3                    0.857143  vp22]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vp_nb_gainloss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create preprocessed dataframe with all vp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Merge dataframe list to single dataframe. \"inner\": Just take columns which exist in all dataframes    \n",
    "gainloss_df = pd.concat(df_list, ignore_index = True, join = 'inner')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gain/Loss Model fit - individual subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_sets_split = {}\n",
    "param_sets_split['0'] = ['mag_diff','prob_diff']\n",
    "param_sets_split['1'] = ['mag_diff','prob_diff','sqrt_prop_revealed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "-1L",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-93dce55b8509>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mparamnum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'1'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[1;31m# Fit a model to the ambiguous gain trials\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_model_split_amb_unamb_gain_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhichreturn\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparam_sets_split\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mparamnum\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mzscore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Hanna\\ambi_gain_loss_shock\\functions\\Scripts_LogRegModels_v2.py\u001b[0m in \u001b[0;36mfit_model_split_amb_unamb_gain_loss\u001b[0;34m(trial_table, cross_validate, combined, split_gain_loss, whichreturn, params, zscore)\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pred_y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0myhat\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m     \u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'MID'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMID\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m     \u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'params'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pvalues'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Hanna\\Anaconda2\\lib\\site-packages\\pandas\\core\\series.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Hanna\\Anaconda2\\lib\\site-packages\\pandas\\indexes\\base.pyc\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   1978\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1979\u001b[0m             return self._engine.get_value(s, k,\n\u001b[0;32m-> 1980\u001b[0;31m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[1;32m   1981\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1982\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minferred_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'integer'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'boolean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_value (pandas\\index.c:3332)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_value (pandas\\index.c:3035)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas\\index.c:4018)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.Int64HashTable.get_item (pandas\\hashtable.c:6610)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.Int64HashTable.get_item (pandas\\hashtable.c:6554)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: -1L"
     ]
    }
   ],
   "source": [
    "vp_list = ['10'] #'06','07','10', '12', '13', '15', '16', '17', '18', '19', '20', '22', '23_2', '25_2', '26_2', '27_2', '28_2', '29', '30']\n",
    "resultnames = ['bic','aic','pseudoR2','pred_acc']\n",
    "model_param_df = np.array(['','','',3.0])\n",
    "model_summary_df = nobrainer_gainloss\n",
    "\n",
    "for vp in vp_list:\n",
    "    \n",
    "    df = gainloss_df[gainloss_df.MID == 'vp' + vp]\n",
    "    MID = 'vp' + vp\n",
    "        \n",
    "    for split in ['ambig_gain','ambig_loss','unambig_gain','unambig_loss']:\n",
    "        #print(split)\n",
    "        if 'unambig' in split:\n",
    "            paramnum='0'\n",
    "        else:\n",
    "            paramnum='1'\n",
    "        # Fit a model to the ambiguous gain trials \n",
    "        out = fit_model_split_amb_unamb_gain_loss(df, whichreturn= split, params=param_sets_split[paramnum],zscore=False)\n",
    "        \n",
    "    \n",
    "        modelname = out['modelname']\n",
    "\n",
    "        for result in resultnames:\n",
    "            model_summary_df.loc[(model_summary_df.MID== 'vp' + vp),result+'_'+split]=out[result]\n",
    "\n",
    "        params = out['params']\n",
    "        for param in params.index:\n",
    "            paramn = param.replace('_loss','')\n",
    "            paramn = paramn.replace('_gain','')\n",
    "            paramn = paramn.replace('_amb','')\n",
    "            paramn = paramn.replace('_rl','')\n",
    "            row = np.array([MID,paramn,split,params[param]])\n",
    "            model_param_df=np.vstack((model_param_df,row))\n",
    "\n",
    "model_param_df = pd.DataFrame(model_param_df,columns=['MID','parameter','split','beta'])\n",
    "model_param_df.drop(0,inplace=True) #df.index[0]\n",
    "model_param_df['beta']=model_param_df['beta'].astype('float')\n",
    "\n",
    "print(out['aic'])\n",
    "print(out['pred_acc'])\n",
    "print(out['modelname'])\n",
    "out['results'].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "899    vp10\n",
       "Name: MID, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.MID[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gainloss_ls_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit group of gain loss data\n",
    "redundant to what I did before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for vp in vp_list:\n",
    "    \n",
    "    path = os.path.join(os.getcwd(),'..','data','data_gainloss_logfiles','vp' + vp + '_gainloss_processed.csv')\n",
    "    df = pd.read_csv(path, sep=\",\")\n",
    "    #preprocess gainloss data\n",
    "    df=preprocess_gainloss(df)\n",
    "    #preprocess data\n",
    "    df = preprocess(df)\n",
    "\n",
    "    for split in ['ambig_shock','ambig_gain','ambig_loss']:\n",
    "        out = fit_model_split_amb_unamb_gain_loss(trial_table,\n",
    "                                                  whichreturn=split,\n",
    "                                                  params=param_sets_split['1'],zscore=False)\n",
    "        modelname = out['modelname']\n",
    "        print(modelname)\n",
    "        print(out['params'])\n",
    "        #pickle.dump(out,open(savefolder+\n",
    "                #MID+'_'+modelname+'.p', \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# shock load in single subjectdata + nobrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_list = []\n",
    "vp_list = ['06', '07', '10', '11', '12', '13', '15', '16', '17', '18', '19', '20', '22', '23', '25', '26', '27', '28', '29', '30']\n",
    "vp_perform_shock_list = []\n",
    "vp_nb_shock_list = []\n",
    "section_list = ['1', '2', '3']\n",
    "for vp in vp_list:\n",
    "    df = []\n",
    "    for sec in section_list:\n",
    "        path = os.path.join(os.getcwd(),'..','data','data_shock_logfiles','Expt1Pain_Behaviour_vp' + vp + '_' + sec + '.txt')\n",
    "        df_dummy = pd.read_csv(path, sep=\"\\t\", skiprows = [0])\n",
    "        df_dummy = df_dummy[:-1] #deletes last row of each section as it does not contain trial data\n",
    "        df_dummy['MID'] = 'vp'+ vp\n",
    "        df_dummy['section'] = sec\n",
    "        df_dummy.columns = df_dummy.columns.str.replace(' ','')\n",
    "        df.append(df_dummy)\n",
    "    \n",
    "    #create a df that contains data from all sections    \n",
    "    df = pd.concat(df, ignore_index = True, join = 'inner')\n",
    "    #preprocess shock data\n",
    "    df = preprocess_shock(df)\n",
    "    #preprocess data\n",
    "    df = preprocess(df)\n",
    "    #store prepocessed data in list that contains data for all subjects (for later analysis)\n",
    "    df_list.append(df)\n",
    "    #create subset with unambiguous trials for no brainer analysis\n",
    "    nb_df = drop_ambi_trials(df)\n",
    "    #create variables indicating whether left or right was the better option\n",
    "    better_choice_shock(nb_df)\n",
    "    #indicate whether the better box was chosen\n",
    "    nb_df = right_choice(nb_df)\n",
    "    #only keep trials that are 'no brainers'\n",
    "    nb_df = keep_nobrainers(nb_df)\n",
    "    #calculate performance\n",
    "    vp_perform_shock = ['vp' + vp, vp_perf(nb_df)]\n",
    "    #store each vp performance in list\n",
    "    vp_perform_shock_list.append(vp_perform_shock)\n",
    "    #vp performance sectionwise\n",
    "    vp_nb_shock = nb_df.groupby('section').mean().add_prefix('shock_')[['shock_choseBetter']]\n",
    "    vp_nb_shock['MID'] = 'vp'+ vp\n",
    "    vp_nb_shock_list.append(vp_nb_shock)\n",
    "    \n",
    "#make dataframe for nb performance\n",
    "nobrainer_shock = pd.DataFrame(vp_perform_shock_list,columns=['MID','nbperf'])\n",
    "\n",
    "#dataframe for single subject (last one) to try out code\n",
    "shock_ls_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_test = drop_ambi_trials(df)\n",
    "    \n",
    "better_choice_shock(df_test)\n",
    "    \n",
    "df_test = right_choice(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_df[['mag_left', 'mag_right', 'prob_x_l', 'prob_x_r', 'resp', 'left_better', 'right_better', 'choseBetter']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vp_nb_shock_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create preprocessed dataframe with all vp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create complete df for shock condition with all vps        \n",
    "shock_df = pd.concat(df_list, ignore_index = True, join = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shock: Model fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vp_list = ['06', '07', '10', '11' ,'12', '13', '15', '16', '17', '18', '19', '20', '22', '23_2', '25_2', '26_2', '27_2', '28_2', '29', '30']\n",
    "resultnames = ['bic','aic','pseudoR2','pred_acc'] ##'llr_pvalue' is missing bc did not work\n",
    "model_param_df = np.array(['','','',3.0])\n",
    "model_summary_df = nobrainer_shock\n",
    "\n",
    "for vp in vp_list:\n",
    "    \n",
    "    df = shock_df[shock_df.MID == 'vp' + vp]\n",
    "    MID = 'vp' + vp\n",
    "        \n",
    "    for split in ['ambig_gain','ambig_loss','unambig_gain','unambig_loss']:\n",
    "        #print(split)\n",
    "        if 'unambig' in split:\n",
    "            paramnum='0'\n",
    "        else:\n",
    "            paramnum='1'\n",
    "        # Fit a model to the ambiguous gain trials \n",
    "        out = fit_model_split_amb_unamb_gain_loss(df, whichreturn= split, params=param_sets_split[paramnum],zscore=False)\n",
    "        \n",
    "    \n",
    "        modelname = out['modelname']\n",
    "\n",
    "        for result in resultnames:\n",
    "            model_summary_df.loc[(model_summary_df.MID== 'vp' + vp),result+'_'+split]=out[result]\n",
    "\n",
    "        params = out['params']\n",
    "        for param in params.index:\n",
    "            paramn = param.replace('_loss','')\n",
    "            paramn = paramn.replace('_gain','')\n",
    "            paramn = paramn.replace('_amb','')\n",
    "            paramn = paramn.replace('_rl','')\n",
    "            row = np.array([MID,paramn,split,params[param]])\n",
    "            model_param_df=np.vstack((model_param_df,row))\n",
    "\n",
    "model_param_df = pd.DataFrame(model_param_df,columns=['MID','parameter','split','beta'])\n",
    "model_param_df.drop(0,inplace=True) #df.index[0]\n",
    "model_param_df['beta']=model_param_df['beta'].astype('float')\n",
    "\n",
    "print(out['aic'])\n",
    "print(out['pred_acc'])\n",
    "print(out['modelname'])\n",
    "out['results'].summary()\n",
    "    \n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit a model to the unambiguous gain trials \n",
    "out = fit_model_split_amb_unamb_gain_loss(gainloss_ls_df,\n",
    "                                          whichreturn='unambig_gain', # specifies which section of trials\n",
    "                                          params=param_sets_split['0'], # specifies which model parameters to fit\n",
    "                                          zscore=False)\n",
    "print(out['aic'])\n",
    "print(out['pred_acc'])\n",
    "print(out['modelname'])\n",
    "out['results'].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit a model to the ambiguous shock trials \n",
    "\n",
    "out = fit_model_split_amb_unamb_gain_loss(df,\n",
    "                                          whichreturn='ambig_shock', # specifies which section of trials\n",
    "                                          params=param_sets_split['1'], # specifies which model parameters to fit\n",
    "                                          zscore=False)\n",
    "print(out['aic'])\n",
    "print(out['pred_acc'])\n",
    "print(out['modelname'])\n",
    "out['results'].summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load subject fits and Plot Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "See Example_analyze_emmas_fmri_shock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nobrainer_shock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nobrainer_gainloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vp_nb_gainloss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
